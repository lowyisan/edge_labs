{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EENlZvOtPDZ6"
      },
      "source": [
        "# Quantization tutorial\n",
        "\n",
        "This tutorial shows how to do post-training static quantization, as well as illustrating two more advanced techniques - per-channel quantization and quantization-aware training - to further improve the model’s accuracy. The task is to classify MNIST digits with a simple LeNet architecture.\n",
        "\n",
        "This is a minimalistic tutorial to show you a starting point for quantization in PyTorch. For theory and more in-depth explanations, please check out: [Quantizing deep convolutional networks for efficient inference: A whitepaper](https://arxiv.org/abs/1806.08342).\n",
        "\n",
        "The tutorial is heavily adapted from: [Static Quantization Tutorial](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n",
        "\n",
        "**Top-Level Explanation:** This notebook demonstrates how to prepare, calibrate, and convert a simple CNN for MNIST digit classification to a quantized model. It covers data loading, model training, post-training quantization, and quantization-aware training (QAT).\n",
        "\n",
        "**Potential Lab Q&A:**\n",
        "- *Q: What is post-training quantization?*\n",
        "  *A: It is the process of converting a pre-trained floating point model to a quantized version without retraining.*\n",
        "- *Q: What are QuantStub and DeQuantStub used for?*\n",
        "  *A: They mark the boundaries for quantization and dequantization in the network, allowing for efficient inference with int8 arithmetic.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvIwDlYvBzC"
      },
      "source": [
        "### Initial Setup\n",
        "\n",
        "Before beginning the assignment, we import the MNIST dataset and train a simple convolutional neural network (CNN) to classify it. The following code installs specific versions of torch and torchvision, then imports necessary libraries for building and quantizing the model.\n",
        "\n",
        "**Potential Lab Q&A:**\n",
        "- *Q: Why install specific versions of torch?*\n",
        "  *A: To ensure compatibility with the quantization code and to replicate the tutorial results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbiiMcdNJI--",
        "outputId": "b03a637b-c757-47d9-ff23-1846dfe8c63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5.0 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.5.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
         ]
        }
      ],
      "source": [
        "# Install specific versions of torch and torchvision\n",
        "!pip3 install torch==1.5.0 torchvision==1.6.0\n",
        "\n",
        "# Import necessary libraries for deep learning and quantization\n",
        "import torch  # PyTorch for building and training the neural network\n",
        "import torchvision  # Contains vision datasets, models, and transforms\n",
        "import torchvision.transforms as transforms  # For preprocessing images\n",
        "import torch.nn as nn  # Neural network module\n",
        "import torch.nn.functional as F  # Functional interface for NN operations\n",
        "import torch.optim as optim  # Optimizers for training the network\n",
        "import os  # Operating system interface\n",
        "from torch.utils.data import DataLoader  # Utility for loading data in batches\n",
        "import torch.quantization  # Tools for quantizing the model\n",
        "from torch.quantization import QuantStub, DeQuantStub  # Stubs for quantization and dequantization\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: What is quantization?\n",
        "# A: Quantization reduces the precision of weights and activations to reduce model size and increase inference speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaMDWYArEXO"
      },
      "source": [
        "Load training and test data from the MNIST dataset and apply a normalizing transformation.\n",
        "\n",
        "**Key Point:** MNIST images are normalized to center the data, which helps with training stability.\n",
        "\n",
        "**Potential Q&A:**\n",
        "- *Q: Why do we normalize the images?*\n",
        "  *A: Normalization scales the pixel values to a standard range, improving model convergence during training.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5UuOjjrnogR",
        "outputId": "f91bf88b-1f86-4777-d3cc-4ff31f629fbe"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 343kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.23MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.7MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define a transformation pipeline for MNIST images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean 0.5 and std 0.5 (since MNIST is grayscale)\n",
        "])\n",
        "\n",
        "# Load the MNIST training dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=16, pin_memory=True)\n",
        "\n",
        "# Load the MNIST test dataset\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: Why use pin_memory in DataLoader?\n",
        "# A: pin_memory=True speeds up data transfer to GPU by using page-locked memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG5qXPDxnUnj"
      },
      "source": [
        "Define some helper functions and classes that help us to track the statistics and accuracy with respect to the train/test data.\n",
        "\n",
        "**Key Points:**\n",
        "- `AverageMeter` is used to compute and store the average and current values of metrics like loss and accuracy.\n",
        "- `accuracy` computes the top-1 accuracy of the model.\n",
        "- `print_size_of_model` prints the size of the model (which is important to check quantization effectiveness).\n",
        "\n",
        "**Potential Q&A:**\n",
        "- *Q: Why do we fuse modules in a quantized model?*\n",
        "  *A: Fusing layers like Conv+ReLU reduces memory access and improves numerical accuracy after quantization.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WetzHpQybN1k"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    \"\"\"Computes the top 1 accuracy.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(1, 1, True, True)  # Get top prediction for each example\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
        "        return correct_one.mul_(100.0 / batch_size).item()\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    \"\"\"Prints the size of the model in MB.\"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def load_model(quantized_model, model):\n",
        "    \"\"\"Loads the weights from a trained model into a quantized model object.\"\"\"\n",
        "    state_dict = model.state_dict()\n",
        "    model = model.to('cpu')\n",
        "    quantized_model.load_state_dict(state_dict)\n",
        "\n",
        "def fuse_modules(model):\n",
        "    \"\"\"Fuses convolution/linear layers with ReLU for better quantization efficiency.\"\"\"\n",
        "    torch.quantization.fuse_modules(model, [['conv1', 'relu1'],\n",
        "                                            ['conv2', 'relu2'],\n",
        "                                            ['fc1', 'relu3'],\n",
        "                                            ['fc2', 'relu4']], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62CkyIwtSOv"
      },
      "source": [
        "Define a simple CNN that classifies MNIST images.\n",
        "\n",
        "**Key Points:**\n",
        "- The network is a minimal LeNet-like CNN.\n",
        "- Quantization stubs are included if quantization is enabled (`q=True`).\n",
        "\n",
        "**Potential Q&A:**\n",
        "- *Q: What is the purpose of using `reshape` instead of `view`?*\n",
        "  *A: `reshape` is more flexible and can handle non-contiguous tensors, reducing potential errors during flattening.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, q=False):\n",
        "        # If q is True, quantization is enabled\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, bias=False)  # First convolutional layer\n",
        "        self.relu1 = nn.ReLU()  # Activation\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # Pooling layer\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)  # Second convolutional layer\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256, 120, bias=False)  # First fully connected layer\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84, bias=False)  # Second fully connected layer\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10, bias=False)  # Output layer\n",
        "        self.q = q\n",
        "        if q:\n",
        "            # QuantStub marks the beginning of quantization\n",
        "            self.quant = QuantStub()\n",
        "            # DeQuantStub marks the end of quantization\n",
        "            self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.q:\n",
        "            x = self.quant(x)  # Quantize the input\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        # Flatten the tensor; use reshape instead of view for safety\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc3(x)\n",
        "        if self.q:\n",
        "            x = self.dequant(x)  # Dequantize the output\n",
        "        return x\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: What are QuantStub and DeQuantStub used for?\n",
        "# A: They define the boundaries where the model converts between floating point and quantized representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9_LdxSTb3BJ",
        "outputId": "540e9fbe-f3a4-416b-8938-de64035b7252"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size (MB): 0.179057\n"
          ]
        }
      ],
      "source": [
        "net = Net(q=False).cuda()  # Create an instance of the CNN and move it to the GPU\n",
        "print_size_of_model(net)  # Print the size of the model (important to compare before/after quantization)\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: Why is model size important in quantization?\n",
        "# A: Quantization can reduce model size, which is crucial for deploying models on resource-constrained devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nijieuxptag6"
      },
      "source": [
        "Train this CNN on the training dataset (this may take a few moments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, cuda=False, q=False):\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Optimizer\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(10):  # Train for 10 epochs\n",
        "\n",
        "        running_loss = AverageMeter('loss')\n",
        "        acc = AverageMeter('train_acc')\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # Get the inputs and labels\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "            if epoch >= 3 and q:\n",
        "                model.apply(torch.quantization.disable_observer)  # Disable observers after 3 epochs if using QAT\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            # Update running loss and accuracy statistics\n",
        "            running_loss.update(loss.item(), outputs.shape[0])\n",
        "            acc.update(accuracy(outputs, labels), outputs.shape[0])\n",
        "            if i % 100 == 0:  # Log every 100 mini-batches\n",
        "                print('[%d, %5d] ' % (epoch + 1, i + 1), running_loss, acc)\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass during testing\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total  # Return accuracy as a percentage\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: How is accuracy computed in the test function?\n",
        "# A: By comparing the predicted labels with the true labels and computing the percentage of correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HixhBHaqtmZU",
        "outputId": "22964b09-97bf-46f3-e592-5f33755021eb"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,     1]  loss 2.301825 (2.301825) train_acc 9.375000 (9.375000)\n",
            "[1,   101]  loss 2.296545 (2.299753) train_acc 14.062500 (13.180693)\n",
            "[1,   201]  loss 2.294123 (2.295533) train_acc 20.312500 (16.969838)\n",
            "[1,   301]  loss 2.260918 (2.289851) train_acc 31.250000 (19.684385)\n",
            "[1,   401]  loss 2.229809 (2.280524) train_acc 28.125000 (22.241272)\n",
            "[1,   501]  loss 2.112481 (2.260747) train_acc 34.375000 (23.924027)\n",
            "[1,   601]  loss 1.691649 (2.207187) train_acc 56.250000 (26.000936)\n",
            "[1,   701]  loss 1.302064 (2.094674) train_acc 57.812500 (30.683845)\n",
            "[1,   801]  loss 0.603729 (1.944274) train_acc 79.687500 (35.927747)\n",
            "[1,   901]  loss 0.637056 (1.799278) train_acc 79.687500 (40.822697)\n",
            "[2,     1]  loss 0.421752 (0.421752) train_acc 84.375000 (84.375000)\n",
            "[2,   101]  loss 0.555698 (0.466051) train_acc 76.562500 (84.916460)\n",
            "[2,   201]  loss 0.367405 (0.443531) train_acc 90.625000 (85.828669)\n",
            "[2,   301]  loss 0.338774 (0.414789) train_acc 87.500000 (86.908223)\n",
            "[2,   401]  loss 0.418230 (0.392866) train_acc 85.937500 (87.612999)\n",
            "[2,   501]  loss 0.192535 (0.373555) train_acc 93.750000 (88.254741)\n",
            "[2,   601]  loss 0.267304 (0.359256) train_acc 90.625000 (88.779118)\n",
            "[2,   701]  loss 0.270399 (0.347294) train_acc 89.062500 (89.138285)\n",
            "[2,   801]  loss 0.217331 (0.334197) train_acc 95.312500 (89.565777)\n",
            "[2,   901]  loss 0.210387 (0.323724) train_acc 92.187500 (89.901845)\n",
            "[3,     1]  loss 0.179686 (0.179686) train_acc 93.750000 (93.750000)\n",
            "[3,   101]  loss 0.177956 (0.218159) train_acc 96.875000 (93.301361)\n",
            "[3,   201]  loss 0.131281 (0.212642) train_acc 96.875000 (93.625622)\n",
            "[3,   301]  loss 0.230773 (0.202948) train_acc 93.750000 (93.864203)\n",
            "[3,   401]  loss 0.160824 (0.198201) train_acc 95.312500 (94.030549)\n",
            "[3,   501]  loss 0.164737 (0.193917) train_acc 93.750000 (94.142964)\n",
            "[3,   601]  loss 0.279176 (0.192728) train_acc 92.187500 (94.121776)\n",
            "[3,   701]  loss 0.065058 (0.189104) train_acc 98.437500 (94.251516)\n",
            "[3,   801]  loss 0.084926 (0.184719) train_acc 98.437500 (94.387875)\n",
            "[3,   901]  loss 0.197474 (0.180851) train_acc 93.750000 (94.490497)\n",
            "[4,     1]  loss 0.052120 (0.052120) train_acc 98.437500 (98.437500)\n",
            "[4,   101]  loss 0.144741 (0.153190) train_acc 93.750000 (95.281559)\n",
            "[4,   201]  loss 0.058057 (0.144277) train_acc 100.000000 (95.545709)\n",
            "[4,   301]  loss 0.194177 (0.142744) train_acc 93.750000 (95.546096)\n",
            "[4,   401]  loss 0.065646 (0.140524) train_acc 96.875000 (95.624221)\n",
            "[4,   501]  loss 0.154308 (0.139291) train_acc 93.750000 (95.671158)\n",
            "[4,   601]  loss 0.096717 (0.137997) train_acc 96.875000 (95.718074)\n",
            "[4,   701]  loss 0.079012 (0.135662) train_acc 96.875000 (95.793955)\n",
            "[4,   801]  loss 0.095808 (0.134334) train_acc 95.312500 (95.870396)\n",
            "[4,   901]  loss 0.041427 (0.133144) train_acc 100.000000 (95.903857)\n",
            "[5,     1]  loss 0.122821 (0.122821) train_acc 93.750000 (93.750000)\n",
            "[5,   101]  loss 0.150619 (0.116678) train_acc 96.875000 (96.426361)\n",
            "[5,   201]  loss 0.217060 (0.120113) train_acc 95.312500 (96.284204)\n",
            "[5,   301]  loss 0.095003 (0.115354) train_acc 98.437500 (96.480482)\n",
            "[5,   401]  loss 0.178125 (0.113512) train_acc 92.187500 (96.516521)\n",
            "[5,   501]  loss 0.102078 (0.109734) train_acc 95.312500 (96.644212)\n",
            "[5,   601]  loss 0.105195 (0.108953) train_acc 98.437500 (96.612417)\n",
            "[5,   701]  loss 0.085071 (0.108335) train_acc 96.875000 (96.647646)\n",
            "[5,   801]  loss 0.170604 (0.109351) train_acc 92.187500 (96.580446)\n",
            "[5,   901]  loss 0.100234 (0.108733) train_acc 96.875000 (96.588860)\n",
            "[6,     1]  loss 0.121567 (0.121567) train_acc 95.312500 (95.312500)\n",
            "[6,   101]  loss 0.075490 (0.094121) train_acc 98.437500 (97.230817)\n",
            "[6,   201]  loss 0.064948 (0.089618) train_acc 96.875000 (97.193719)\n",
            "[6,   301]  loss 0.145593 (0.092476) train_acc 95.312500 (97.087832)\n",
            "[6,   401]  loss 0.225213 (0.093565) train_acc 92.187500 (97.050343)\n",
            "[6,   501]  loss 0.046396 (0.092195) train_acc 98.437500 (97.093313)\n",
            "[6,   601]  loss 0.098693 (0.092610) train_acc 95.312500 (97.116785)\n",
            "[6,   701]  loss 0.063563 (0.093459) train_acc 98.437500 (97.100125)\n",
            "[6,   801]  loss 0.087230 (0.092648) train_acc 96.875000 (97.122737)\n",
            "[6,   901]  loss 0.103449 (0.091624) train_acc 96.875000 (97.157672)\n",
            "[7,     1]  loss 0.120803 (0.120803) train_acc 93.750000 (93.750000)\n",
            "[7,   101]  loss 0.027874 (0.093506) train_acc 100.000000 (96.967822)\n",
            "[7,   201]  loss 0.047028 (0.085835) train_acc 98.437500 (97.178172)\n",
            "[7,   301]  loss 0.049162 (0.082928) train_acc 98.437500 (97.347384)\n",
            "[7,   401]  loss 0.040157 (0.082934) train_acc 98.437500 (97.369857)\n",
            "[7,   501]  loss 0.061389 (0.082282) train_acc 98.437500 (97.433258)\n",
            "[7,   601]  loss 0.107987 (0.084587) train_acc 95.312500 (97.363769)\n",
            "[7,   701]  loss 0.136759 (0.084009) train_acc 96.875000 (97.374287)\n",
            "[7,   801]  loss 0.036806 (0.082340) train_acc 98.437500 (97.421192)\n",
            "[7,   901]  loss 0.060247 (0.081543) train_acc 98.437500 (97.454218)\n",
            "[8,     1]  loss 0.090545 (0.090545) train_acc 96.875000 (96.875000)\n",
            "[8,   101]  loss 0.044258 (0.073398) train_acc 98.437500 (97.725866)\n",
            "[8,   201]  loss 0.049968 (0.077063) train_acc 98.437500 (97.714552)\n",
            "[8,   301]  loss 0.085269 (0.078063) train_acc 96.875000 (97.658846)\n",
            "[8,   401]  loss 0.165036 (0.079706) train_acc 93.750000 (97.560786)\n",
            "[8,   501]  loss 0.171269 (0.078292) train_acc 95.312500 (97.632859)\n",
            "[8,   601]  loss 0.014902 (0.078714) train_acc 100.000000 (97.634151)\n",
            "[8,   701]  loss 0.018776 (0.076614) train_acc 100.000000 (97.693028)\n",
            "[8,   801]  loss 0.077634 (0.075756) train_acc 95.312500 (97.688436)\n",
            "[8,   901]  loss 0.072130 (0.074623) train_acc 96.875000 (97.702206)\n",
            "[9,     1]  loss 0.023216 (0.023216) train_acc 100.000000 (100.000000)\n",
            "[9,   101]  loss 0.025362 (0.075065) train_acc 100.000000 (97.787748)\n",
            "[9,   201]  loss 0.023575 (0.075787) train_acc 100.000000 (97.776741)\n",
            "[9,   301]  loss 0.056463 (0.071825) train_acc 98.437500 (97.767857)\n",
            "[9,   401]  loss 0.015321 (0.068792) train_acc 98.437500 (97.868610)\n",
            "[9,   501]  loss 0.022236 (0.067967) train_acc 100.000000 (97.922904)\n",
            "[9,   601]  loss 0.057575 (0.068147) train_acc 98.437500 (97.896735)\n",
            "[9,   701]  loss 0.082051 (0.066788) train_acc 98.437500 (97.924840)\n",
            "[9,   801]  loss 0.064518 (0.067079) train_acc 95.312500 (97.906913)\n",
            "[9,   901]  loss 0.059543 (0.067299) train_acc 96.875000 (97.905105)\n",
            "[10,     1]  loss 0.066471 (0.066471) train_acc 98.437500 (98.437500)\n",
            "[10,   101]  loss 0.049779 (0.061040) train_acc 98.437500 (98.220916)\n",
            "[10,   201]  loss 0.012248 (0.062884) train_acc 100.000000 (98.041045)\n",
            "[10,   301]  loss 0.017566 (0.060775) train_acc 100.000000 (98.105274)\n",
            "[10,   401]  loss 0.074730 (0.061761) train_acc 98.437500 (98.153055)\n",
            "[10,   501]  loss 0.009322 (0.062308) train_acc 100.000000 (98.110030)\n",
            "[10,   601]  loss 0.028268 (0.062351) train_acc 100.000000 (98.122920)\n",
            "[10,   701]  loss 0.056530 (0.062900) train_acc 95.312500 (98.092011)\n",
            "[10,   801]  loss 0.147855 (0.062269) train_acc 96.875000 (98.117587)\n",
            "[10,   901]  loss 0.045884 (0.062466) train_acc 98.437500 (98.101068)\n",
            "Finished Training\n"
         ]
        }
      ],
      "source": [
        "train(net, trainloader, cuda=True)  # Train the CNN on the MNIST training data\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: Why do we use SGD with momentum for training?\n",
        "# A: Momentum helps accelerate gradients vectors in the right directions, leading to faster convergences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJggxnCVuRxU"
      },
      "source": [
        "Now that the CNN has been trained, let's test it on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y27_n-djuEdz",
        "outputId": "8e67d498-b18d-4be9-95fe-7e8706d23c9e"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 98.09% - FP32\n"
          ]
        }
      ],
      "source": [
        "score = test(net, testloader, cuda=True)  # Evaluate the trained network on the test data\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: What does FP32 indicate?\n",
        "# A: FP32 indicates that the network is using 32-bit floating point precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lp-ElDsrKua"
      },
      "source": [
        "### Post-training quantization\n",
        "\n",
        "Define a new quantized network architecture, where we also define the quantization and dequantization stubs that will be important at the start and at the end.\n",
        "\n",
        "Next, we’ll “fuse modules”; this can both make the model faster by saving on memory access while also improving numerical accuracy. While this can be used with any model, this is especially common with quantized models.\n",
        "\n",
        "**Process Overview:**\n",
        "1. **Prepare:** Insert observers into the model to collect activation statistics (min/max values).\n",
        "2. **Calibration:** Run the model on representative data to calibrate these observers.\n",
        "3. **Convert:** Use the collected statistics to compute quantization parameters and convert the model's operations to quantized versions.\n",
        "\n",
        "**Potential Q&A:**\n",
        "- *Q: What is the purpose of fusing modules?*\n",
        "  *A: It reduces memory access and improves numerical accuracy after quantization.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X-nQWDXrhItv"
      },
      "outputs": [],
      "source": [
        "qnet = Net(q=True)  # Create a quantized version of the network (with quantization stubs)\n",
        "load_model(qnet, net)  # Load the pretrained floating point weights into the quantized model\n",
        "fuse_modules(qnet)  # Fuse layers (e.g., Conv+ReLU) to optimize the model for quantization\n",
        "\n",
        "# Inline Q&A:\n",
        "# Q: Why do we need to load weights into qnet?\n",
        "# A: We need to initialize the quantized network with the trained weights from the floating point model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiaQkj6wJuC6"
      },
      "source": [
        "In general, we have the following process (Post Training Quantization):\n",
        "\n",
        "1. **Prepare:** Insert observers into the model to observe activation statistics.\n",
        "2. **Calibration:** Run the model on representative sample data to collect statistics.\n",
        "3. **Convert:** Compute quantization parameters and convert the model from floating point to quantized operators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-ZaMV4bUb6-",
        "outputId": "77098546-d269-44d8-a1b6-6d60cac12029"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Post Training Quantization Prepare: Inserting Observers\n",
            "\n",
            " Conv1: After observer insertion \n",
            "\n",
            " ConvReLU2d(\n",
            "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (1): ReLU()\n",
            "  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
            ")\n",
            "Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "\n",
            " Conv1: After fusion and quantization \n",
            "\n",
            " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05912807211279869, zero_point=0, bias=False)\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n"
          ]
        }
      ],
      "source": [
        "qnet.qconfig = torch.quantization.default_qconfig  # Set the default quantization configuration\n",
        "print(qnet.qconfig)  # Print the quantization configuration (for verification)\n",
        "torch.quantization.prepare(qnet, inplace=True)  # Prepare the model by inserting observers\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "print('\\n Conv1: After observer insertion \\n\\n', qnet.conv1)  # Display the first convolution layer after observer insertion\n",
        "\n",
        "test(qnet, trainloader, cuda=False)  # Run calibration using training data\n",
        "print('Post Training Quantization: Calibration done')\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert the calibrated model to a quantized version\n",
        "print('Post Training Quantization: Convert done')\n",
        "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)  # Display the fused and quantized conv layer\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)  # Print model size to observe reduction due to quantization\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: What does calibration do in post-training quantization?\n",
        "# A: It collects activation statistics from representative data to determine optimal quantization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbDvGBtMavCO",
        "outputId": "9a1bb06b-dee0-4293-ac8a-05c4a13d7868"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the fused and quantized network on the test images: 98.11% - INT8\n"
          ]
        }
      ],
      "source": [
        "score = test(qnet, testloader, cuda=False)  # Evaluate the quantized model on test data\n",
        "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: How does quantization affect accuracy?\n",
        "# A: Proper quantization can maintain high accuracy while reducing model size and latency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcv6Gi45lZ4L"
      },
      "source": [
        "We can also define a custom quantization configuration, where we replace the default observers and instead of quantizing with respect to max/min we can take an average of the observed max/min, hopefully for a better generalization performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNj6TNFu1ljn",
        "outputId": "054066c3-0c61-4a94-df74-3d6dcd71f328"
     },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "\n",
            " Conv1: After fusion and quantization \n",
            "\n",
            " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05865493789315224, zero_point=0, bias=False)\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n",
            "Accuracy of the fused and quantized network on the test images: 98.13% - INT8\n"
          ]
        }
      ],
      "source": [
        "from torch.quantization.observer import MovingAverageMinMaxObserver  # Import a custom observer for quantization\n",
        "\n",
        "qnet = Net(q=True)  # Create a quantized network\n",
        "load_model(qnet, net)  # Load pretrained weights into the quantized model\n",
        "fuse_modules(qnet)  # Fuse modules (e.g., Conv+ReLU) for improved quantization\n",
        "\n",
        "qnet.qconfig = torch.quantization.QConfig(\n",
        "    activation=MovingAverageMinMaxObserver.with_args(reduce_range=True),\n",
        "    weight=MovingAverageMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
        ")\n",
        "print(qnet.qconfig)  # Print the custom quantization configuration\n",
        "torch.quantization.prepare(qnet, inplace=True)  # Insert observers into the model for QAT calibration\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "print('\\n Conv1: After observer insertion \\n\\n', qnet.conv1)  # Inspect the conv layer after observer insertion\n",
        "\n",
        "test(qnet, trainloader, cuda=False)  # Run calibration on training data\n",
        "print('Post Training Quantization: Calibration done')\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert the model to its quantized version\n",
        "print('Post Training Quantization: Convert done')\n",
        "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)  # Inspect the quantized conv layer\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)  # Print model size to verify reduction\n",
        "score = test(qnet, testloader, cuda=False)  # Evaluate the quantized model on test data\n",
        "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: What does QConfig specify?\n",
        "# A: It specifies the quantization observers for activations and weights, determining how quantization parameters are computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LXNCT7fgcMx"
      },
      "source": [
        "In addition, we can significantly improve on the accuracy simply by using a different quantization configuration. We repeat the same exercise with the recommended configuration for quantizing for arm64 architecture (qnnpack). This configuration does the following:\n",
        "\n",
        "- Quantizes weights on a per-channel basis.\n",
        "- Uses a histogram observer that collects a histogram of activations and then picks quantization parameters in an optimal manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-nZq5yF_gWBs"
      },
      "outputs": [],
      "source": [
        "qnet = Net(q=True)  # Reinitialize quantized network for custom config\n",
        "load_model(qnet, net)  # Load pretrained weights\n",
        "fuse_modules(qnet)  # Fuse modules before applying new quantization configuration\n",
        "\n",
        "# Inline Q&A:\n",
        "# Q: Why reinitialize qnet for a custom configuration?\n",
        "# A: To apply a different quantization setup, we need to start from a fresh quantized network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXv5pAwVlGFh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n"
          ]
        }
      ],
      "source": [
        "qnet.qconfig = torch.quantization.get_default_qconfig('qnnpack')  # Use the default QNNPACK config for arm64\n",
        "print(qnet.qconfig)  # Print the new quantization configuration\n",
        "\n",
        "torch.quantization.prepare(qnet, inplace=True)  # Prepare model with new observers\n",
        "test(qnet, trainloader, cuda=False)  # Run calibration\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert to quantized model\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)  # Print model size after applying new configuration\n",
        "\n",
        "# Potential Q&A:\n",
        "# Q: What does get_default_qconfig('qnnpack') do?\n",
        "# A: It returns a quantization configuration optimized for ARM architectures using QNNPACK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "X5Vjyayimv8n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the fused and quantized network on the test images: 98.02% - INT8\n"
          ]
        }
      ],
      "source": [
        "score = test(qnet, testloader, cuda=False)  # Evaluate the quantized network on the test dataset\n",
        "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))\n",
        "\n",
        "# Inline Q&A:\n",
        "# Q: How is the accuracy affected after quantization?\n",
        "# A: With proper calibration and conversion, accuracy remains high (here, around 98% using INT8 precision)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A_G3tsasU6U"
      },
      "source": [
        "### Quantization aware training\n",
        "\n",
        "Quantization-aware training (QAT) is the quantization method that typically results in the highest accuracy. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers.\n",
        "\n",
        "**Key Points:**\n",
        "- QAT simulates quantization effects during training so the model learns to adapt to quantized weights and activations.\n",
        "- This usually results in better accuracy compared to post-training quantization.\n",
        "\n",
        "**Potential Q&A:**\n",
        "- *Q: What is the main advantage of QAT?*\n",
        "  *A: QAT typically yields higher accuracy because the network adjusts its weights during training to account for quantization effects.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o-mGba7QsXzf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Conv1: After fusion and quantization \n",
            "\n",
            " ConvReLU2d(\n",
            "  1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False\n",
            "  (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
            "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
            "    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "  )\n",
            "  (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
            "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
            "    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "  )\n",
            ")\n",
            "[1,     1]  loss 2.304440 (2.304440) train_acc 9.375000 (9.375000)\n",
            "...\n",
            "Finished Training\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n",
            "Accuracy of the fused and quantized network (trained quantized) on the test images: 98.0% - INT8\n"
          ]
        }
      ],
      "source": [
        "qnet = Net(q=True)  # Create a quantized network for QAT\n",
        "fuse_modules(qnet)  # Fuse modules for optimal QAT performance\n",
        "qnet.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')  # Set QAT configuration for arm64\n",
        "torch.quantization.prepare_qat(qnet, inplace=True)  # Prepare the model for QAT by inserting fake quantization modules\n",
        "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
        "qnet = qnet.cuda()  # Move model to GPU for training\n",
        "train(qnet, trainloader, cuda=True)  # Train the model with quantization-aware training enabled\n",
        "qnet = qnet.cpu()  # Move the trained model back to CPU\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert the QAT model to a fully quantized (INT8) model\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)  # Print the size of the quantized model\n",
        "\n",
        "score = test(qnet, testloader, cuda=False)  # Evaluate the quantized model on test data\n",
        "print('Accuracy of the fused and quantized network (trained quantized) on the test images: {}% - INT8'.format(score))\n",
        "\n",
        "# Inline Q&A:\n",
        "# Q: What is the purpose of QAT?\n",
        "# A: QAT simulates quantization during training to help the network adjust its weights, resulting in better accuracy after conversion to INT8."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
