{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EENlZvOtPDZ6"
      },
      "source": [
        "# Quantization tutorial\n",
        "\n",
        "This tutorial shows how to do post-training static quantization, as well as illustrating two more advanced techniques – per-channel quantization and quantization-aware training – to further improve the model’s accuracy. The task is to classify MNIST digits with a simple LeNet architecture.\n",
        "\n",
        "This is a minimalistic tutorial to show you a starting point for quantization in PyTorch. For theory and more in-depth explanations, please check out: [Quantizing deep convolutional networks for efficient inference: A whitepaper](https://arxiv.org/abs/1806.08342).\n",
        "\n",
        "The tutorial is heavily adapted from: https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\n",
        "\n",
        "### Top-Level Explanation\n",
        "\n",
        "This notebook demonstrates how to improve the inference efficiency of a neural network by quantizing a CNN trained on the MNIST dataset. It covers three key methods:\n",
        "  1. **Post-Training Static Quantization:** Inserting observers, calibrating on sample data, and converting to quantized operators.\n",
        "  2. **Custom Quantization Configuration:** Using alternative observers (e.g. moving average observers) to potentially improve generalization.\n",
        "  3. **Quantization Aware Training (QAT):** Training the network while simulating quantization effects to achieve higher accuracy in the quantized model.\n",
        "\n",
        "Potential Lab Test Q&A:\n",
        "  - **Q:** What is the purpose of quantization in deep learning?\n",
        "    **A:** Quantization reduces model size and increases inference speed by converting weights and activations from floating point to lower precision (e.g. int8) while trying to preserve accuracy.\n",
        "  - **Q:** How does quantization-aware training differ from post-training quantization?\n",
        "    **A:** QAT simulates quantization effects during training, which usually results in better accuracy than post-training quantization that only converts a pretrained model.\n",
        "Quantization-aware training (QAT) integrates the quantization process into the training loop. This means that during training, the network simulates the effects of quantization—such as rounding errors and limited precision—in its forward and backward passes. As a result, the model learns to compensate for these inaccuracies, adjusting its weights to maintain performance under quantized conditions.\n",
        "\n",
        "On the other hand, post-training quantization (PTQ) converts a fully-trained, full-precision model to lower precision after training is complete. Since the model wasn't exposed to quantization effects during training, it may not be as robust to the quantization errors introduced during conversion. This often leads to a larger drop in accuracy compared to QAT.\n",
        "\n",
        "In summary, QAT usually results in better accuracy because it allows the model to learn and adapt to the quantization errors during training, while PTQ applies quantization retrospectively without any adaptive compensation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvIwDlYvBzC"
      },
      "source": [
        "### Initial Setup\n",
        "\n",
        "Before beginning the assignment, we import the MNIST dataset, and train a simple convolutional neural network (CNN) to classify it. In the next cells, we install and import the required libraries and define helper functions and classes for training, evaluation, and quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbiiMcdNJI--",
        "outputId": "b03a637b-c757-47d9-ff23-1846dfe8c63b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5.0 (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.5.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install specific versions of PyTorch and torchvision (for compatibility with the tutorial code)\n",
        "!pip3 install torch==1.5.0 torchvision==1.6.0\n",
        "\n",
        "# Import essential PyTorch packages and quantization tools\n",
        "import torch                # Core PyTorch library\n",
        "import torchvision          # Provides datasets and models for computer vision tasks\n",
        "import torchvision.transforms as transforms  # For data preprocessing and augmentation\n",
        "import torch.nn as nn       # For building neural network modules\n",
        "import torch.nn.functional as F  # Provides functional interface for common operations\n",
        "import torch.optim as optim # Optimizers for training\n",
        "import os                   # For file and directory operations\n",
        "from torch.utils.data import DataLoader  # For loading and batching datasets\n",
        "import torch.quantization   # For post-training quantization tools\n",
        "from torch.quantization import QuantStub, DeQuantStub  # For inserting quantization and dequantization steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaMDWYArEXO"
      },
      "source": [
        "Load training and test data from the MNIST dataset and apply a normalizing transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5UuOjjrnogR",
        "outputId": "f91bf88b-1f86-4777-d3cc-4ff31f629fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 343kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.23MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Define a transformation: convert images to tensors and normalize them\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize MNIST images (grayscale) with mean=0.5, std=0.5\n",
        "])\n",
        "\n",
        "# Load MNIST training dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=16, pin_memory=True)\n",
        "\n",
        "# Load MNIST test dataset\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=16, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG5qXPDxnUnj"
      },
      "source": [
        "Define some helper functions and classes that help us track training statistics and compute accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WetzHpQybN1k"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Helper class to compute and store the average and current value.\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def accuracy(output, target):\n",
        "    \"\"\"Computes the top-1 accuracy for the given output and target labels.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(1, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
        "        return correct_one.mul_(100.0 / batch_size).item()\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    \"\"\"Saves the model temporarily and prints its size in MB.\"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\") / 1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "def load_model(quantized_model, model):\n",
        "    \"\"\"Loads pretrained weights from the original model into the quantized model.\"\"\"\n",
        "    state_dict = model.state_dict()\n",
        "    model = model.to('cpu')\n",
        "    quantized_model.load_state_dict(state_dict)\n",
        "\n",
        "def fuse_modules(model):\n",
        "    \"\"\"Fuses convolution/linear layers with their subsequent ReLU activation for improved performance and accuracy.\"\"\"\n",
        "    torch.quantization.fuse_modules(model, [['conv1', 'relu1'],\n",
        "                                            ['conv2', 'relu2'],\n",
        "                                            ['fc1', 'relu3'],\n",
        "                                            ['fc2', 'relu4']], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62CkyIwtSOv"
      },
      "source": [
        "Define a simple CNN (LeNet-style) to classify MNIST images. This network optionally includes quantization stubs to enable quantization-aware training or post-training quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, q=False):\n",
        "        # If q is True, quantization stubs are added for quantization-aware training or post-training quantization\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256, 120, bias=False)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
        "        self.q = q\n",
        "        if q:\n",
        "            self.quant = QuantStub()    # Marks the beginning of quantization\n",
        "            self.dequant = DeQuantStub()  # Marks the end of quantization\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.q:\n",
        "            x = self.quant(x)  # Quantize the input\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.reshape(x.shape[0], -1)  # Flatten the tensor for the fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc3(x)\n",
        "        if self.q:\n",
        "            x = self.dequant(x)  # Dequantize the output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9_LdxSTb3BJ",
        "outputId": "540e9fbe-f3a4-416b-8938-de64035b7252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size (MB): 0.179057\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the network (without quantization for initial training)\n",
        "net = Net(q=False).cuda()\n",
        "print_size_of_model(net)  # Print model size to benchmark before quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nijieuxptag6"
      },
      "source": [
        "Train this CNN on the training dataset (this may take a few moments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, cuda=False, q=False):\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # SGD optimizer\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(10):  # Train for 10 epochs\n",
        "\n",
        "        running_loss = AverageMeter('loss')\n",
        "        acc = AverageMeter('train_acc')\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, labels = data  # Get inputs and labels\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "            if epoch >= 3 and q:\n",
        "                model.apply(torch.quantization.disable_observer)  # Disable observers after a few epochs when quantization is enabled\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize weights\n",
        "\n",
        "            running_loss.update(loss.item(), outputs.shape[0])\n",
        "            acc.update(accuracy(outputs, labels), outputs.shape[0])\n",
        "            if i % 100 == 0:\n",
        "                print('[%d, %5d] ' % (epoch + 1, i + 1), running_loss, acc)\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total  # Return accuracy percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HixhBHaqtmZU",
        "outputId": "22964b09-97bf-46f3-e592-5f33755021eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,     1]  loss 2.301825 (2.301825) train_acc 9.375000 (9.375000)\n",
            "... (training log output) ...\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Train the network on the MNIST training set\n",
        "train(net, trainloader, cuda=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJggxnCVuRxU"
      },
      "source": [
        "Now that the CNN has been trained, let's test it on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y27_n-djuEdz",
        "outputId": "8e67d498-b18d-4be9-95fe-7e8706d23c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 98.09% - FP32\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the network on the MNIST test set\n",
        "score = test(net, testloader, cuda=True)\n",
        "print('Accuracy of the network on the test images: {}% - FP32'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lp-ElDsrKua"
      },
      "source": [
        "### Post-training quantization\n",
        "\n",
        "Define a new quantized network architecture, which includes quantization and dequantization stubs. Then, fuse modules to both speed up the model and improve numerical accuracy. This process involves:\n",
        "\n",
        "1. **Prepare:** Inserting observers into the model to record activation statistics.\n",
        "2. **Calibration:** Running the model on sample data to gather tensor statistics.\n",
        "3. **Convert:** Converting floating-point operations to quantized operations using the recorded statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X-nQWDXrhItv"
      },
      "outputs": [],
      "source": [
        "# Create a quantized network instance by enabling quantization (q=True)\n",
        "qnet = Net(q=True)\n",
        "load_model(qnet, net)   # Load the pretrained weights into the quantized network\n",
        "fuse_modules(qnet)      # Fuse layers to improve efficiency and accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiaQkj6wJuC6"
      },
      "source": [
        "The following code prepares the model for post-training quantization. Observers are inserted, calibration is performed, and the model is converted to a quantized version.\n",
        "\n",
        "Potential Lab Test Q&A:\n",
        "  - **Q:** What is the role of observers in post-training quantization?\n",
        "    **A:** Observers record the range (min/max values) of activations, which are later used to determine the quantization parameters.\n",
        "  - **Q:** Why do we fuse modules before converting to a quantized model?\n",
        "    **A:** Fusion reduces memory accesses and computation overhead by merging operations (e.g., Conv + ReLU), which improves both speed and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-ZaMV4bUb6-",
        "outputId": "77098546-d269-44d8-a1b6-6d60cac12029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Post Training Quantization Prepare: Inserting Observers\n",
            "\n",
            " Conv1: After observer insertion \n",
            "\n",
            " ConvReLU2d(\n",
            "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (1): ReLU()\n",
            "  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
            ")\n",
            "Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "\n",
            " Conv1: After fusion and quantization \n",
            "\n",
            " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05912807211279869, zero_point=0, bias=False)\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n"
          ]
        }
      ],
      "source": [
        "qnet.qconfig = torch.quantization.default_qconfig  # Use the default quantization configuration\n",
        "print(qnet.qconfig)  # Print the QConfig to verify settings\n",
        "torch.quantization.prepare(qnet, inplace=True)  # Insert observers into the model\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "print('\\n Conv1: After observer insertion \\n\\n', qnet.conv1)\n",
        "\n",
        "test(qnet, trainloader, cuda=False)  # Run calibration on the training set\n",
        "print('Post Training Quantization: Calibration done')\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert the model to use quantized operators\n",
        "print('Post Training Quantization: Convert done')\n",
        "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbDvGBtMavCO",
        "outputId": "9a1bb06b-dee0-4293-ac8a-05c4a13d7868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the fused and quantized network on the test images: 98.11% - INT8\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the quantized model on the test dataset\n",
        "score = test(qnet, testloader, cuda=False)\n",
        "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcv6Gi45lZ4L"
      },
      "source": [
        "We can also define a custom quantization configuration. In this configuration, we replace the default observers with ones that use a moving average to calculate min/max values, which may improve generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNj6TNFu1ljn",
        "outputId": "054066c3-0c61-4a94-df74-3d6dcd71f328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Post Training Quantization Prepare: Inserting Observers\n",
            "\n",
            " Conv1: After observer insertion \n",
            "\n",
            " ConvReLU2d(\n",
            "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "  (1): ReLU()\n",
            "  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            ")\n",
            "Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "\n",
            " Conv1: After fusion and quantization \n",
            "\n",
            " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.05865493789315224, zero_point=0, bias=False)\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n",
            "Accuracy of the fused and quantized network on the test images: 98.13% - INT8\n"
          ]
        }
      ],
      "source": [
        "from torch.quantization.observer import MovingAverageMinMaxObserver\n",
        "\n",
        "qnet = Net(q=True)  # Create a new quantized network instance\n",
        "load_model(qnet, net)  # Load pretrained weights\n",
        "fuse_modules(qnet)  # Fuse layers as before\n",
        "\n",
        "qnet.qconfig = torch.quantization.QConfig(\n",
        "    activation=MovingAverageMinMaxObserver.with_args(reduce_range=True),\n",
        "    weight=MovingAverageMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
        ")\n",
        "print(qnet.qconfig)\n",
        "\n",
        "torch.quantization.prepare(qnet, inplace=True)  # Insert the custom observers\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "print('\\n Conv1: After observer insertion \\n\\n', qnet.conv1)\n",
        "\n",
        "test(qnet, trainloader, cuda=False)  # Calibrate using training data\n",
        "print('Post Training Quantization: Calibration done')\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert to quantized model\n",
        "print('Post Training Quantization: Convert done')\n",
        "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)\n",
        "\n",
        "score = test(qnet, testloader, cuda=False)  # Evaluate quantized model\n",
        "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LXNCT7fgcMx"
      },
      "source": [
        "In addition, we can significantly improve accuracy by using a different quantization configuration (qnnpack) optimized for arm64 architectures. This configuration quantizes weights per channel and uses a histogram observer to select optimal quantization parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-nZq5yF_gWBs"
      },
      "outputs": [],
      "source": [
        "qnet = Net(q=True)\n",
        "load_model(qnet, net)\n",
        "fuse_modules(qnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXv5pAwVlGFh",
        "outputId": "8a112f5f-020f-4138-eff9-7aaf9c463479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n"
          ]
        }
      ],
      "source": [
        "qnet.qconfig = torch.quantization.get_default_qconfig('qnnpack')  # Use qnnpack config for ARM architectures\n",
        "print(qnet.qconfig)\n",
        "\n",
        "torch.quantization.prepare(qnet, inplace=True)  # Insert observers\n",
        "test(qnet, trainloader, cuda=False)\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert the model\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Vjyayimv8n",
        "outputId": "2b63ad28-821e-4193-80c8-ff9ebbd3a30c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the fused and quantized network on the test images: 98.02% - INT8\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using the qnnpack quantization configuration\n",
        "score = test(qnet, testloader, cuda=False)\n",
        "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A_G3tsasU6U"
      },
      "source": [
        "### Quantization Aware Training\n",
        "\n",
        "Quantization-aware training (QAT) simulates quantization effects during both forward and backward passes. This typically leads to higher accuracy in the final quantized model since the network learns to compensate for the quantization error during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o-mGba7QsXzf",
        "outputId": "c18ca8b2-0d3e-4abe-d89b-a98778395224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Conv1: After fusion and quantization \n",
            "\n",
            " ConvReLU2d(\n",
            "  1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False\n",
            "  (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
            "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
            "    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "  )\n",
            "  (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
            "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
            "    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
            "  )\n",
            ")\n",
            "[... training log output ...]\n",
            "Finished Training\n",
            "Size of model after quantization\n",
            "Size (MB): 0.050084\n",
            "Accuracy of the fused and quantized network (trained quantized) on the test images: 98.0% - INT8\n"
          ]
        }
      ],
      "source": [
        "qnet = Net(q=True)  # Create a new network for QAT with quantization stubs enabled\n",
        "fuse_modules(qnet)  # Fuse the appropriate modules\n",
        "qnet.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')  # Set the QAT configuration\n",
        "torch.quantization.prepare_qat(qnet, inplace=True)  # Prepare the model for quantization-aware training by inserting fake quantization modules\n",
        "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
        "qnet = qnet.cuda()  # Move model to GPU for training\n",
        "train(qnet, trainloader, cuda=True)  # Train the quantization-aware model\n",
        "qnet = qnet.cpu()  # Move model back to CPU for conversion\n",
        "torch.quantization.convert(qnet, inplace=True)  # Convert the QAT model to a fully quantized model\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(qnet)\n",
        "\n",
        "score = test(qnet, testloader, cuda=False)  # Evaluate the quantized model\n",
        "print('Accuracy of the fused and quantized network (trained quantized) on the test images: {}% - INT8'.format(score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
